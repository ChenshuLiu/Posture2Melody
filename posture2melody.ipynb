{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating landmark and mel-spectrogram data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import moviepy.editor as movp\n",
    "import os\n",
    "import warnings\n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "import pandas as pd\n",
    "# mediapipe\n",
    "mp_drawing = mp.solutions.drawing_utils #visualizing poses using visual indicators\n",
    "mp_pose = mp.solutions.pose #pose estimation models (solutions)\n",
    "# extracting audio from video (ffmpeg)\n",
    "os.environ[\"IMAGEIO_FFMPEG_EXE\"] = \"/opt/homebrew/opt/ffmpeg/bin/ffmpeg\"\n",
    "\n",
    "def Euc_dist(pts, cm, img_h, img_w):\n",
    "        distances_sq = (pts - cm) ** 2\n",
    "        euc_distances = distances_sq.sum(axis = 1)\n",
    "        diag_length = np.sqrt(img_h**2 + img_w**2)\n",
    "        return euc_distances * diag_length\n",
    "\n",
    "def landmark_data_gen(vid_dir, landmark_dir):\n",
    "    # suppress warning messages\n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "    landmarks = [f'landmark_{i}' for i in range(0, 33)]\n",
    "    features = ['x', 'y', 'distance_to_cm']\n",
    "    comprehensive_landmarks_df_columns = pd.MultiIndex.from_product([landmarks, features], names=['landmark', 'feature'])\n",
    "    comprehensive_landmarks_df = pd.DataFrame(columns = comprehensive_landmarks_df_columns)\n",
    "\n",
    "    cap = cv2.VideoCapture(vid_dir)\n",
    "    with mp_pose.Pose(min_detection_confidence=0.5, # using the pose estimation model\n",
    "                    min_tracking_confidence=0.5) as pose:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read() # frame is the actual image of each frame\n",
    "\n",
    "            if not ret: # ending the loop when the video is done playing\n",
    "                break\n",
    "            \n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image.flags.writeable = False #not modifiable\n",
    "            results = pose.process(image) #making the actual detection\n",
    "            image.flags.writeable = True #now modifiable\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) #opencv wants image file in BGR format, we need to rerender the images\n",
    "            image_h, image_w, _ = image.shape\n",
    "            \n",
    "            if results.pose_landmarks: #sometimes cannot capture any landmarks (mediapipe requires full body view for processing)\n",
    "                landmarks = results.pose_landmarks.landmark #list of pose landmarks\n",
    "\n",
    "                # convert landmark object into dataframe\n",
    "                landmarks_df = pd.DataFrame(columns=['x', 'y'])\n",
    "                for idx, landmark in enumerate(landmarks):\n",
    "                    new_entry = np.array([landmark.x, landmark.y])\n",
    "                    landmarks_df.loc[idx,:] = new_entry\n",
    "\n",
    "                ######## For All landmarks #########\n",
    "                # for all landmark points, keep track of their relative position w.r.t to center of mass (important for posture reconstruction)\n",
    "                all_cm_x, all_cm_y = landmarks_df['x'].mean(), landmarks_df['y'].mean()\n",
    "                landmarks_df['distance_to_cm'] = Euc_dist(np.array(landmarks_df.loc[:, ['x', 'y']]), \n",
    "                                                        np.array([all_cm_x, all_cm_y]),\n",
    "                                                        image_h, image_w)\n",
    "\n",
    "                ######## Storing all important features #########\n",
    "                # x, y coordinates, distance of each landmark from the center of mass\n",
    "                new_data = pd.DataFrame(columns=comprehensive_landmarks_df_columns)\n",
    "                for landmark_idx in range(33):\n",
    "                    landmark_name = f\"landmark_{landmark_idx}\"\n",
    "                    for feature_idx in range(len(features)):\n",
    "                        feature_name = features[feature_idx]\n",
    "                        new_data[(landmark_name, feature_name)] = [landmarks_df.loc[:, feature_name].iloc[landmark_idx]]\n",
    "                comprehensive_landmarks_df = pd.concat([comprehensive_landmarks_df, new_data], ignore_index=True)\n",
    "\n",
    "                # finding and visualizing center of mass\n",
    "                cm_x, cm_y = landmarks_df['x'].mean(), landmarks_df['y'].mean()\n",
    "                cm_x_coord, cm_y_coord = int(cm_x * image_w), int(cm_y * image_h)\n",
    "                cv2.circle(img = image, \n",
    "                        center = (cm_x_coord, cm_y_coord), \n",
    "                        radius=5, \n",
    "                        color = (0, 0, 255), \n",
    "                        thickness = -1)\n",
    "\n",
    "            else:\n",
    "                pass #when landmarks are not detected\n",
    "\n",
    "            mp_drawing.draw_landmarks(image = image, \n",
    "                                    landmark_list = results.pose_landmarks, #coordinate of each landmark detected by the pose estimation model\n",
    "                                    connections = mp_pose.POSE_CONNECTIONS, #the connections of each pose landmark\n",
    "                                    landmark_drawing_spec = mp_drawing.DrawingSpec(color = (245, 117, 66), thickness=2, circle_radius=2),\n",
    "                                    connection_drawing_spec = mp_drawing.DrawingSpec(color = (245, 117, 66), thickness=2, circle_radius=2))\n",
    "\n",
    "            cv2.imshow('Webcam Feed', image)\n",
    "            \n",
    "            if cv2.waitKey(10) & 0xFF == ord('q'): #0xFF is checking which key is being pressed\n",
    "                break\n",
    "\n",
    "        np.save(landmark_dir, comprehensive_landmarks_df)\n",
    "        cap.release() #release the use of webcame\n",
    "        cv2.destroyAllWindows() #close all cv2 associated windows\n",
    "        cv2.waitKey(1) #need this line!! else the mediapipe window maynot close properly for running on local vscode (basically giving the kernel enough time to close the windows)\n",
    "\n",
    "    # Re-enable warnings\n",
    "    warnings.filterwarnings('default')\n",
    "\n",
    "def audio_data_gen(vid_dir, audio_dir, mel_dir):\n",
    "\n",
    "    video = movp.VideoFileClip(vid_dir)\n",
    "\n",
    "    # Extract the audio from the video\n",
    "    audio_path = audio_dir\n",
    "    video.audio.write_audiofile(audio_path)\n",
    "\n",
    "    # create spectrogram based on audio\n",
    "    y, sr = librosa.load(audio_path)\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=2048, hop_length=512).T # [mel_bins, n_segments] --> [n_segments, mel_bins]\n",
    "    np.save(mel_dir, mel_spectrogram) # saving the numpy array\n",
    "\n",
    "def data_generation(main_dir):\n",
    "    vid_folder = os.path.join(main_dir, 'video')\n",
    "\n",
    "    for vid_file in os.listdir(vid_folder):\n",
    "        if vid_file != \".DS_Store\":\n",
    "            vid_dir = os.path.join(vid_folder, vid_file)\n",
    "            vid_name = vid_file.split('.')[0]\n",
    "\n",
    "            # create landmark data\n",
    "            landmark_dir = os.path.join(main_dir, 'landmark', f'{vid_name}_landmark.npy')\n",
    "            landmark_data_gen(vid_dir, landmark_dir)\n",
    "\n",
    "            # create mel_spectrogram data\n",
    "            audio_dir = os.path.join(main_dir, 'audio', f'{vid_name}_audio.wav')\n",
    "            mel_dir = os.path.join(main_dir, 'mel_spectrogram', f'{vid_name}_mel_spectrogram.npy')\n",
    "            audio_data_gen(vid_dir, audio_dir, mel_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generation('/Users/liuchenshu/Documents/Research/Human Posture/mediadata')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Option: Unify the length for each batch (not across the entire dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/liuchenshu/Documents/Research/Human Posture/mediadata/landmark/A good time on vacation _Alexander A ！_landmark.npy', '/Users/liuchenshu/Documents/Research/Human Posture/mediadata/landmark/Love you _Alexander A !_landmark.npy', \"/Users/liuchenshu/Documents/Research/Human Posture/mediadata/landmark/Let's have a party！ #dance _Alexander A !_landmark.npy\", '/Users/liuchenshu/Documents/Research/Human Posture/mediadata/landmark/Shining in the darkness of the night_landmark.npy', '/Users/liuchenshu/Documents/Research/Human Posture/mediadata/landmark/Let me tell you a secret…_landmark.npy', '/Users/liuchenshu/Documents/Research/Human Posture/mediadata/landmark/Break through yourself ！_landmark.npy', '/Users/liuchenshu/Documents/Research/Human Posture/mediadata/landmark/The Walker Out of the Night！ Alexander A !_landmark.npy', '/Users/liuchenshu/Documents/Research/Human Posture/mediadata/landmark/scorching summer day _Alexander A ！_landmark.npy', '/Users/liuchenshu/Documents/Research/Human Posture/mediadata/landmark/call me_Alexander A_landmark.npy', \"/Users/liuchenshu/Documents/Research/Human Posture/mediadata/landmark/Dance music that's getting to the top_landmark.npy\", '/Users/liuchenshu/Documents/Research/Human Posture/mediadata/landmark/midnight _Alexander A！_landmark.npy', '/Users/liuchenshu/Documents/Research/Human Posture/mediadata/landmark/Classic reappearance_landmark.npy', '/Users/liuchenshu/Documents/Research/Human Posture/mediadata/landmark/Sparkling Snap Dance_Alexander A！_landmark.npy', '/Users/liuchenshu/Documents/Research/Human Posture/mediadata/landmark/become progressively uncontrollable_landmark.npy', '/Users/liuchenshu/Documents/Research/Human Posture/mediadata/landmark/Get in the groove and start moving  _ Alexander A ！_landmark.npy', '/Users/liuchenshu/Documents/Research/Human Posture/mediadata/landmark/bad boy Alexander A_landmark.npy', '/Users/liuchenshu/Documents/Research/Human Posture/mediadata/landmark/Today you are the master of the stage!_landmark.npy', '/Users/liuchenshu/Documents/Research/Human Posture/mediadata/landmark/Bomb the street_Entertainment only_Alexander A !!_landmark.npy', '/Users/liuchenshu/Documents/Research/Human Posture/mediadata/landmark/Simple Snap Dance, for those who go above and beyond!_Alexander A ！_landmark.npy', '/Users/liuchenshu/Documents/Research/Human Posture/mediadata/landmark/Please come to center stage^ ^_Alexander A！_landmark.npy', '/Users/liuchenshu/Documents/Research/Human Posture/mediadata/landmark/return of the king！！_Alexander A_landmark.npy', '/Users/liuchenshu/Documents/Research/Human Posture/mediadata/landmark/Breaking the Rhythm_ Alexander A !_landmark.npy', '/Users/liuchenshu/Documents/Research/Human Posture/mediadata/landmark/When you come to center stage_Alexander A !_landmark.npy', '/Users/liuchenshu/Documents/Research/Human Posture/mediadata/landmark/Shining in the darkness～～_landmark.npy', '/Users/liuchenshu/Documents/Research/Human Posture/mediadata/landmark/Fly with me!_Alexander  A_landmark.npy', '/Users/liuchenshu/Documents/Research/Human Posture/mediadata/landmark/Something different_landmark.npy', '/Users/liuchenshu/Documents/Research/Human Posture/mediadata/landmark/The Black Elf  _Alexander A ！_landmark.npy', '/Users/liuchenshu/Documents/Research/Human Posture/mediadata/landmark/A memorable season_landmark.npy', '/Users/liuchenshu/Documents/Research/Human Posture/mediadata/landmark/The macho man is coming_landmark.npy', '/Users/liuchenshu/Documents/Research/Human Posture/mediadata/landmark/kicking back and relaxing^ ^_Alexander  A！_landmark.npy', '/Users/liuchenshu/Documents/Research/Human Posture/mediadata/landmark/Dancing lightly with everyone_landmark.npy', '/Users/liuchenshu/Documents/Research/Human Posture/mediadata/landmark/Freedom comes from being together!_landmark.npy', '/Users/liuchenshu/Documents/Research/Human Posture/mediadata/landmark/I Need U_landmark.npy']\n",
      "['/Users/liuchenshu/Documents/Research/Human Posture/mediadata/mel_spectrogram/bad boy Alexander A_mel_spectrogram.npy', '/Users/liuchenshu/Documents/Research/Human Posture/mediadata/mel_spectrogram/Please come to center stage^ ^_Alexander A！_mel_spectrogram.npy', '/Users/liuchenshu/Documents/Research/Human Posture/mediadata/mel_spectrogram/When you come to center stage_Alexander A !_mel_spectrogram.npy', '/Users/liuchenshu/Documents/Research/Human Posture/mediadata/mel_spectrogram/midnight _Alexander A！_mel_spectrogram.npy', '/Users/liuchenshu/Documents/Research/Human Posture/mediadata/mel_spectrogram/Break through yourself ！_mel_spectrogram.npy', '/Users/liuchenshu/Documents/Research/Human Posture/mediadata/mel_spectrogram/I Need U_mel_spectrogram.npy', '/Users/liuchenshu/Documents/Research/Human Posture/mediadata/mel_spectrogram/Freedom comes from being together!_mel_spectrogram.npy', '/Users/liuchenshu/Documents/Research/Human Posture/mediadata/mel_spectrogram/The Walker Out of the Night！ Alexander A !_mel_spectrogram.npy', '/Users/liuchenshu/Documents/Research/Human Posture/mediadata/mel_spectrogram/Shining in the darkness of the night_mel_spectrogram.npy', '/Users/liuchenshu/Documents/Research/Human Posture/mediadata/mel_spectrogram/A memorable season_mel_spectrogram.npy', '/Users/liuchenshu/Documents/Research/Human Posture/mediadata/mel_spectrogram/Dancing lightly with everyone_mel_spectrogram.npy', '/Users/liuchenshu/Documents/Research/Human Posture/mediadata/mel_spectrogram/A good time on vacation _Alexander A ！_mel_spectrogram.npy', \"/Users/liuchenshu/Documents/Research/Human Posture/mediadata/mel_spectrogram/Dance music that's getting to the top_mel_spectrogram.npy\", '/Users/liuchenshu/Documents/Research/Human Posture/mediadata/mel_spectrogram/Love you _Alexander A !_mel_spectrogram.npy', '/Users/liuchenshu/Documents/Research/Human Posture/mediadata/mel_spectrogram/The Black Elf  _Alexander A ！_mel_spectrogram.npy', '/Users/liuchenshu/Documents/Research/Human Posture/mediadata/mel_spectrogram/The macho man is coming_mel_spectrogram.npy', '/Users/liuchenshu/Documents/Research/Human Posture/mediadata/mel_spectrogram/Classic reappearance_mel_spectrogram.npy', '/Users/liuchenshu/Documents/Research/Human Posture/mediadata/mel_spectrogram/Today you are the master of the stage!_mel_spectrogram.npy', '/Users/liuchenshu/Documents/Research/Human Posture/mediadata/mel_spectrogram/Fly with me!_Alexander  A_mel_spectrogram.npy', '/Users/liuchenshu/Documents/Research/Human Posture/mediadata/mel_spectrogram/return of the king！！_Alexander A_mel_spectrogram.npy', '/Users/liuchenshu/Documents/Research/Human Posture/mediadata/mel_spectrogram/scorching summer day _Alexander A ！_mel_spectrogram.npy', \"/Users/liuchenshu/Documents/Research/Human Posture/mediadata/mel_spectrogram/Let's have a party！ #dance _Alexander A !_mel_spectrogram.npy\", '/Users/liuchenshu/Documents/Research/Human Posture/mediadata/mel_spectrogram/Get in the groove and start moving  _ Alexander A ！_mel_spectrogram.npy', '/Users/liuchenshu/Documents/Research/Human Posture/mediadata/mel_spectrogram/Something different_mel_spectrogram.npy', '/Users/liuchenshu/Documents/Research/Human Posture/mediadata/mel_spectrogram/Let me tell you a secret…_mel_spectrogram.npy', '/Users/liuchenshu/Documents/Research/Human Posture/mediadata/mel_spectrogram/Sparkling Snap Dance_Alexander A！_mel_spectrogram.npy', '/Users/liuchenshu/Documents/Research/Human Posture/mediadata/mel_spectrogram/become progressively uncontrollable_mel_spectrogram.npy', '/Users/liuchenshu/Documents/Research/Human Posture/mediadata/mel_spectrogram/Bomb the street_Entertainment only_Alexander A !!_mel_spectrogram.npy', '/Users/liuchenshu/Documents/Research/Human Posture/mediadata/mel_spectrogram/Shining in the darkness～～_mel_spectrogram.npy', '/Users/liuchenshu/Documents/Research/Human Posture/mediadata/mel_spectrogram/kicking back and relaxing^ ^_Alexander  A！_mel_spectrogram.npy', '/Users/liuchenshu/Documents/Research/Human Posture/mediadata/mel_spectrogram/Breaking the Rhythm_ Alexander A !_mel_spectrogram.npy', '/Users/liuchenshu/Documents/Research/Human Posture/mediadata/mel_spectrogram/call me_Alexander A_mel_spectrogram.npy', '/Users/liuchenshu/Documents/Research/Human Posture/mediadata/mel_spectrogram/Simple Snap Dance, for those who go above and beyond!_Alexander A ！_mel_spectrogram.npy']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import os\n",
    "\n",
    "class posture2melDataset(Dataset):\n",
    "    def __init__(self, landmark_dir, mel_dir):\n",
    "        self.landmark_dir = landmark_dir\n",
    "        self.mel_dir = mel_dir\n",
    "    \n",
    "    def __len__(self): # return number of samples\n",
    "        return len(self.landmark_dir)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        landmark_data = np.load(self.landmark_dir[idx])\n",
    "        mel_data = np.load(self.mel_dir[idx])\n",
    "        landmark_tensor = torch.tensor(landmark_data, dtype = torch.float32)\n",
    "        mel_tensor = torch.tensor(mel_data, dtype = torch.float32)\n",
    "        return landmark_tensor, mel_tensor\n",
    "\n",
    "# for unifying sequence length in batch\n",
    "def collate_fn(batch):\n",
    "    landmark_data = [item[0] for item in batch]\n",
    "    mel_data = [item[1] for item in batch]\n",
    "    # make sure the sequences are padded according to the longest sequence in the batch\n",
    "    landmark_data_padded = pad_sequence(landmark_data, batch_first=True) # [batch_size, max_seq_length, n_features]\n",
    "    mel_data_padded = pad_sequence(mel_data, batch_first=True)\n",
    "\n",
    "    # for ignoring padding when feeding into transformer (i.d.ing where the paddings are)\n",
    "    landmark_mask = (landmark_data_padded == 0)\n",
    "    mel_mask = (mel_data_padded == 0)\n",
    "\n",
    "    return landmark_data_padded, mel_data_padded, landmark_mask, mel_mask\n",
    "\n",
    "main_dir = '/Users/liuchenshu/Documents/Research/Human Posture/mediadata'  \n",
    "landmark_paths = [os.path.join(main_dir, 'landmark', landmark_name) for landmark_name in os.listdir(os.path.join(main_dir, 'landmark')) if landmark_name != '.DS_Store']\n",
    "print(landmark_paths)\n",
    "mel_paths = [os.path.join(main_dir, 'mel_spectrogram', landmark_name) for landmark_name in os.listdir(os.path.join(main_dir, 'mel_spectrogram')) if landmark_name != '.DS_Store']\n",
    "print(mel_paths)\n",
    "\n",
    "dataset = posture2melDataset(landmark_paths, mel_paths)\n",
    "data_loader = DataLoader(dataset, batch_size = 4, shuffle = True, collate_fn=collate_fn) #collate_fn make sure the window length is same in each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for landmark, mel_spectrogram, landmark_mask, mel_mask in data_loader:\n",
    "    print(landmark.shape)\n",
    "    print(mel_spectrogram.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Option: Unify length according to longest sequence across the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0\n",
      "torch.Size([1, 1867, 99]) torch.Size([1, 1352, 128]) torch.Size([1, 1867]) torch.Size([1, 1352])\n",
      "batch 1\n",
      "torch.Size([1, 1867, 99]) torch.Size([1, 1352, 128]) torch.Size([1, 1867]) torch.Size([1, 1352])\n",
      "batch 2\n",
      "torch.Size([1, 1867, 99]) torch.Size([1, 1352, 128]) torch.Size([1, 1867]) torch.Size([1, 1352])\n",
      "batch 3\n",
      "torch.Size([1, 1867, 99]) torch.Size([1, 1352, 128]) torch.Size([1, 1867]) torch.Size([1, 1352])\n",
      "batch 4\n",
      "torch.Size([1, 1867, 99]) torch.Size([1, 1352, 128]) torch.Size([1, 1867]) torch.Size([1, 1352])\n",
      "batch 5\n",
      "torch.Size([1, 1867, 99]) torch.Size([1, 1352, 128]) torch.Size([1, 1867]) torch.Size([1, 1352])\n",
      "batch 6\n",
      "torch.Size([1, 1867, 99]) torch.Size([1, 1352, 128]) torch.Size([1, 1867]) torch.Size([1, 1352])\n",
      "batch 7\n",
      "torch.Size([1, 1867, 99]) torch.Size([1, 1352, 128]) torch.Size([1, 1867]) torch.Size([1, 1352])\n",
      "batch 8\n",
      "torch.Size([1, 1867, 99]) torch.Size([1, 1352, 128]) torch.Size([1, 1867]) torch.Size([1, 1352])\n",
      "batch 9\n",
      "torch.Size([1, 1867, 99]) torch.Size([1, 1352, 128]) torch.Size([1, 1867]) torch.Size([1, 1352])\n",
      "batch 10\n",
      "torch.Size([1, 1867, 99]) torch.Size([1, 1352, 128]) torch.Size([1, 1867]) torch.Size([1, 1352])\n",
      "batch 11\n",
      "torch.Size([1, 1867, 99]) torch.Size([1, 1352, 128]) torch.Size([1, 1867]) torch.Size([1, 1352])\n",
      "batch 12\n",
      "torch.Size([1, 1867, 99]) torch.Size([1, 1352, 128]) torch.Size([1, 1867]) torch.Size([1, 1352])\n",
      "batch 13\n",
      "torch.Size([1, 1867, 99]) torch.Size([1, 1352, 128]) torch.Size([1, 1867]) torch.Size([1, 1352])\n",
      "batch 14\n",
      "torch.Size([1, 1867, 99]) torch.Size([1, 1352, 128]) torch.Size([1, 1867]) torch.Size([1, 1352])\n",
      "batch 15\n",
      "torch.Size([1, 1867, 99]) torch.Size([1, 1352, 128]) torch.Size([1, 1867]) torch.Size([1, 1352])\n",
      "batch 16\n",
      "torch.Size([1, 1867, 99]) torch.Size([1, 1352, 128]) torch.Size([1, 1867]) torch.Size([1, 1352])\n",
      "batch 17\n",
      "torch.Size([1, 1867, 99]) torch.Size([1, 1352, 128]) torch.Size([1, 1867]) torch.Size([1, 1352])\n",
      "batch 18\n",
      "torch.Size([1, 1867, 99]) torch.Size([1, 1352, 128]) torch.Size([1, 1867]) torch.Size([1, 1352])\n",
      "batch 19\n",
      "torch.Size([1, 1867, 99]) torch.Size([1, 1352, 128]) torch.Size([1, 1867]) torch.Size([1, 1352])\n",
      "batch 20\n",
      "torch.Size([1, 1867, 99]) torch.Size([1, 1352, 128]) torch.Size([1, 1867]) torch.Size([1, 1352])\n",
      "batch 21\n",
      "torch.Size([1, 1867, 99]) torch.Size([1, 1352, 128]) torch.Size([1, 1867]) torch.Size([1, 1352])\n",
      "batch 22\n",
      "torch.Size([1, 1867, 99]) torch.Size([1, 1352, 128]) torch.Size([1, 1867]) torch.Size([1, 1352])\n",
      "batch 23\n",
      "torch.Size([1, 1867, 99]) torch.Size([1, 1352, 128]) torch.Size([1, 1867]) torch.Size([1, 1352])\n",
      "batch 24\n",
      "torch.Size([1, 1867, 99]) torch.Size([1, 1352, 128]) torch.Size([1, 1867]) torch.Size([1, 1352])\n",
      "batch 25\n",
      "torch.Size([1, 1867, 99]) torch.Size([1, 1352, 128]) torch.Size([1, 1867]) torch.Size([1, 1352])\n",
      "batch 26\n",
      "torch.Size([1, 1867, 99]) torch.Size([1, 1352, 128]) torch.Size([1, 1867]) torch.Size([1, 1352])\n",
      "batch 27\n",
      "torch.Size([1, 1867, 99]) torch.Size([1, 1352, 128]) torch.Size([1, 1867]) torch.Size([1, 1352])\n",
      "batch 28\n",
      "torch.Size([1, 1867, 99]) torch.Size([1, 1352, 128]) torch.Size([1, 1867]) torch.Size([1, 1352])\n",
      "batch 29\n",
      "torch.Size([1, 1867, 99]) torch.Size([1, 1352, 128]) torch.Size([1, 1867]) torch.Size([1, 1352])\n",
      "batch 30\n",
      "torch.Size([1, 1867, 99]) torch.Size([1, 1352, 128]) torch.Size([1, 1867]) torch.Size([1, 1352])\n",
      "batch 31\n",
      "torch.Size([1, 1867, 99]) torch.Size([1, 1352, 128]) torch.Size([1, 1867]) torch.Size([1, 1352])\n",
      "batch 32\n",
      "torch.Size([1, 1867, 99]) torch.Size([1, 1352, 128]) torch.Size([1, 1867]) torch.Size([1, 1352])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "class PaddedDataset(Dataset): # unifying across the entire dataset\n",
    "    def __init__(self, input_file_dir, output_file_dir):\n",
    "        # Load the input and output datasets\n",
    "        self.input_file_dir = input_file_dir\n",
    "        self.output_file_dir = output_file_dir\n",
    "        self.inputs = []\n",
    "        for input_dir in self.input_file_dir:\n",
    "            self.inputs.append(np.load(input_dir))\n",
    "        self.outputs = []\n",
    "        for output_dir in self.output_file_dir:\n",
    "            self.outputs.append(np.load(output_dir))\n",
    "\n",
    "        # Calculate the maximum length across the whole dataset for inputs and outputs (NUCLEAR option!!!)\n",
    "        self.max_input_len = max(inp.shape[0] for inp in self.inputs)\n",
    "        self.max_output_len = max(out.shape[0] for out in self.outputs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_file_dir)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_seq = torch.tensor(self.inputs[idx], dtype=torch.float32)\n",
    "        output_seq = torch.tensor(self.outputs[idx], dtype=torch.float32)\n",
    "\n",
    "        # Pad the input sequence to max_input_len\n",
    "        padded_input = torch.cat([input_seq, torch.zeros(self.max_input_len - input_seq.size(0), input_seq.size(1))], dim=0)\n",
    "\n",
    "        # Create input mask\n",
    "        input_mask = (padded_input.sum(dim=1) != 0)  # Shape: (max_input_len, )\n",
    "\n",
    "        # Pad the output sequence to max_output_len\n",
    "        padded_output = torch.cat([output_seq, torch.zeros(self.max_output_len - output_seq.size(0), output_seq.size(1))], dim=0)\n",
    "\n",
    "        # Create output mask\n",
    "        output_mask = (padded_output.sum(dim=1) != 0)  # Shape: (max_output_len, )\n",
    "\n",
    "        return {\n",
    "            'input': padded_input,\n",
    "            'target': padded_output,\n",
    "            'input_mask': input_mask,\n",
    "            'target_mask': output_mask\n",
    "        }\n",
    "\n",
    "main_dir = '/Users/liuchenshu/Documents/Research/Human Posture/mediadata'  \n",
    "landmark_paths = [os.path.join(main_dir, 'landmark', landmark_name) for landmark_name in os.listdir(os.path.join(main_dir, 'landmark')) if landmark_name != '.DS_Store']\n",
    "mel_paths = [os.path.join(main_dir, 'mel_spectrogram', landmark_name) for landmark_name in os.listdir(os.path.join(main_dir, 'mel_spectrogram')) if landmark_name != '.DS_Store']\n",
    "dataset = PaddedDataset(landmark_paths, mel_paths)\n",
    "\n",
    "# Create the DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=1)\n",
    "\n",
    "# Example of iterating over the DataLoader\n",
    "for idx, batch in enumerate(dataloader):\n",
    "    print(f\"batch {idx}\")\n",
    "    inputs = batch['input']  # Shape: (batch_size, max_input_len, input_dim)\n",
    "    targets = batch['target']  # Shape: (batch_size, max_output_len, output_dim)\n",
    "    input_masks = batch['input_mask']  # Shape: (batch_size, max_input_len, 1)\n",
    "    target_masks = batch['target_mask']  # Shape: (batch_size, max_output_len, 1)\n",
    "    print(inputs.shape, targets.shape, input_masks.shape, target_masks.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer + GAN Mel-spectrogram reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, hidden_dim, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        # Create constant 'pe' matrix with values dependent on position and i\n",
    "        pe = torch.zeros(max_len, hidden_dim)  # Initialize a positional encoding matrix of shape (max_len, hidden_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # A column vector [0, 1, ..., max_len-1]\n",
    "        div_term = torch.exp(torch.arange(0, hidden_dim, 2).float() * (-math.log(10000.0) / hidden_dim))\n",
    "        \n",
    "        # Apply sine to even indices in the encoding (2i)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        \n",
    "        # Apply cosine to odd indices in the encoding (2i+1)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Add a batch dimension at the beginning of the positional encoding matrix\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        # Store the positional encodings in the buffer (they are constants, not learnable parameters)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Add positional encoding to the input embeddings (x)\n",
    "        # x is expected to be of shape (batch_size, sequence_length, hidden_dim)\n",
    "        x = x + self.pe[:, :x.size(1), :]  # Add positional encodings to the input embeddings\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, nhead, num_layers, n_features, max_len = 2000):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.embedding = nn.Linear(n_features, hidden_dim)  # Embedding layer\n",
    "        self.pos_encoder = PositionalEncoding(hidden_dim, max_len)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=nhead), \n",
    "            num_layers=num_layers\n",
    "        )\n",
    "\n",
    "    def forward(self, src, src_key_padding_mask=None):\n",
    "        src_emb = self.embedding(src)  # Shape: (batch_size, n_frame1, hidden_dim)\n",
    "        src_emb = self.pos_encoder(src_emb) # adding the position encoding to the semantic encoding\n",
    "        src_emb = src_emb.transpose(0, 1)  # Transpose to (n_frame1, batch_size, hidden_dim)\n",
    "        memory = self.transformer_encoder(src_emb, src_key_padding_mask = src_key_padding_mask)\n",
    "        return memory.transpose(0, 1)  # Transpose back to (batch_size, n_frame1, hidden_dim)\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, nhead, num_layers, mel_dim, max_len = 2000):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.embedding = nn.Linear(mel_dim, hidden_dim)  # Embedding layer\n",
    "        self.pos_encoder = PositionalEncoding(hidden_dim, max_len)\n",
    "        t\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_key_padding_mask = None, memory_key_padding_mask = None):\n",
    "        tgt_emb = self.embedding(tgt)  # Shape: (batch_size, n_frame2, hidden_dim)\n",
    "        tgt_emb = self.pos_encoder(tgt_emb)\n",
    "        tgt_emb = tgt_emb.transpose(0, 1)  # Transpose to (n_frame2, batch_size, hidden_dim)\n",
    "        print(f\"Memory shape from within decoder function: {memory.shape}\")  # Should be (n_frame1, batch_size, hidden_dim)\n",
    "        print(f\"Tgt Embedding shape from within decoder function: {tgt_emb.shape}\")  # Should be (n_frame2, batch_size, hidden_dim)\n",
    "        output = self.transformer_decoder(tgt_emb, memory, \n",
    "                                          tgt_key_padding_mask = tgt_key_padding_mask, \n",
    "                                          memory_key_padding_mask=memory_key_padding_mask)\n",
    "        return output.transpose(0, 1)  # Transpose back to (batch_size, n_frame2, hidden_dim)\n",
    "\n",
    "# class TransformerModel(nn.Module):\n",
    "#     def __init__(self, input_dim, mel_dim, hidden_dim, nhead, num_encoder_layers, num_decoder_layers):\n",
    "#         super(TransformerModel, self).__init__()\n",
    "#         self.encoder = TransformerEncoder(hidden_dim, nhead, num_encoder_layers)\n",
    "#         self.decoder = TransformerDecoder(hidden_dim, nhead, num_decoder_layers)\n",
    "#         self.fc_out = nn.Linear(hidden_dim, mel_dim)  # Final output layer\n",
    "\n",
    "#     def forward(self, src, tgt): # tgt is the groundtruth output\n",
    "#         # Forward through encoder and decoder\n",
    "#         memory = self.encoder(src)\n",
    "#         output = self.decoder(tgt, memory)\n",
    "        \n",
    "#         # Output layer to generate mel spectrograms\n",
    "#         mel_output = self.fc_out(output)  # Shape: (batch_size, n_frame2, mel_dim)\n",
    "#         return mel_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liuchenshu/Documents/Research/Human Posture/humanposturn_venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "device = \"cpu\"\n",
    "# Define model parameters\n",
    "hidden_dim = 256  # Example dimension, adjust as needed\n",
    "nhead = 8  # Number of attention heads\n",
    "num_layers = 4  # Number of transformer layers\n",
    "mel_dim = 128\n",
    "n_features = 99\n",
    "\n",
    "# Instantiate model\n",
    "encoder = TransformerEncoder(hidden_dim, nhead, num_layers, n_features).to(device)\n",
    "decoder = TransformerDecoder(hidden_dim, nhead, num_layers, mel_dim).to(device)\n",
    "transformer_model = nn.Sequential(encoder, decoder).to(device)  # Or define a custom model class that combines them\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Assuming you're predicting continuous values (modify as needed)\n",
    "optimizer = optim.Adam(transformer_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.mps.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input mask shapetorch.Size([1, 1867])\n",
      "memory shape torch.Size([1, 1867, 256])\n",
      "target shape torch.Size([1, 1352, 128])\n",
      "Memory shape from within decoder function: torch.Size([1, 1867, 256])\n",
      "Tgt Embedding shape from within decoder function: torch.Size([1352, 1, 256])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1, 8, 32]' is invalid for input of size 477952",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmemory\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_seq\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtarget_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minput_mask\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Decoder output\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n\u001b[1;32m     30\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, target_seq)  \u001b[38;5;66;03m# Modify if needed based on your output processing\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Research/Human Posture/humanposturn_venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Research/Human Posture/humanposturn_venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[71], line 37\u001b[0m, in \u001b[0;36mTransformerDecoder.forward\u001b[0;34m(self, tgt, memory, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMemory shape from within decoder function: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmemory\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Should be (n_frame1, batch_size, hidden_dim)\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTgt Embedding shape from within decoder function: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtgt_emb\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Should be (n_frame2, batch_size, hidden_dim)\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer_decoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtgt_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Research/Human Posture/humanposturn_venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Research/Human Posture/humanposturn_venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Research/Human Posture/humanposturn_venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:495\u001b[0m, in \u001b[0;36mTransformerDecoder.forward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m    492\u001b[0m tgt_is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(tgt_mask, tgt_is_causal, seq_len)\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 495\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mmemory_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mtgt_is_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_is_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mmemory_is_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_is_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    503\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(output)\n",
      "File \u001b[0;32m~/Documents/Research/Human Posture/humanposturn_venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Research/Human Posture/humanposturn_venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Research/Human Posture/humanposturn_venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:891\u001b[0m, in \u001b[0;36mTransformerDecoderLayer.forward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    890\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sa_block(x, tgt_mask, tgt_key_padding_mask, tgt_is_causal))\n\u001b[0;32m--> 891\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mha_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_is_causal\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    892\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm3(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(x))\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/Documents/Research/Human Posture/humanposturn_venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:909\u001b[0m, in \u001b[0;36mTransformerDecoderLayer._mha_block\u001b[0;34m(self, x, mem, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    907\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_mha_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor, mem: Tensor,\n\u001b[1;32m    908\u001b[0m                attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_causal: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 909\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultihead_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    910\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    911\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    912\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    913\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout2(x)\n",
      "File \u001b[0;32m~/Documents/Research/Human Posture/humanposturn_venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Research/Human Posture/humanposturn_venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Research/Human Posture/humanposturn_venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:1275\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1261\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1262\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[1;32m   1263\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1272\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   1273\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[1;32m   1274\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1275\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1277\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1278\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1279\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[1;32m   1287\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m~/Documents/Research/Human Posture/humanposturn_venv/lib/python3.12/site-packages/torch/nn/functional.py:5466\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   5464\u001b[0m q \u001b[38;5;241m=\u001b[39m q\u001b[38;5;241m.\u001b[39mview(tgt_len, bsz \u001b[38;5;241m*\u001b[39m num_heads, head_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   5465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m static_k \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 5466\u001b[0m     k \u001b[38;5;241m=\u001b[39m \u001b[43mk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbsz\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   5467\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5468\u001b[0m     \u001b[38;5;66;03m# TODO finish disentangling control flow so we don't do in-projections when statics are passed\u001b[39;00m\n\u001b[1;32m   5469\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m static_k\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m==\u001b[39m bsz \u001b[38;5;241m*\u001b[39m num_heads, \\\n\u001b[1;32m   5470\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpecting static_k.size(0) of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbsz\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39mnum_heads\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstatic_k\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[1, 8, 32]' is invalid for input of size 477952"
     ]
    }
   ],
   "source": [
    "num_epochs = 10  # Define number of epochs\n",
    "transformer_model.train()  # Set model to training mode\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0  # Variable to accumulate loss\n",
    "\n",
    "    for batch in dataloader:\n",
    "        # Get padded inputs and targets\n",
    "        input_seq = batch['input'].to(device)  # Shape: (batch_size, max_input_len, input_dim)\n",
    "        target_seq = batch['target'].to(device)  # Shape: (batch_size, max_output_len, output_dim)\n",
    "        input_mask = batch['input_mask'].to(device)  # Input mask\n",
    "        target_mask = batch['target_mask'].to(device)  # Output mask\n",
    "        print(f\"input mask shape{input_mask.shape}\")\n",
    "\n",
    "        # Move tensors to the appropriate device (CPU/GPU)\n",
    "        input_seq = input_seq.to(device)\n",
    "        target_seq = target_seq.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Forward pass through the model\n",
    "            memory = encoder(input_seq, src_key_padding_mask = input_mask)  # Encoder output\n",
    "            print(f\"memory shape {memory.shape}\")\n",
    "            print(f\"target shape {target_seq.shape}\")\n",
    "            output = decoder(target_seq, memory, tgt_key_padding_mask = target_mask, memory_key_padding_mask = input_mask)  # Decoder output\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(output, target_seq)  # Modify if needed based on your output processing\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print average loss for the epoch\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "humanposturn_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
